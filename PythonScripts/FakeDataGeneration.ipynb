{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import csv\n",
    "import datetime,timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define list of states\n",
    "states = ['Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chhattisgarh', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jharkhand', 'Karnataka', 'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya', 'Mizoram', 'Nagaland', 'Odisha', 'Punjab', 'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh', 'Uttarakhand', 'West Bengal']\n",
    "\n",
    "# Define lists of data for enums\n",
    "genders = ['male', 'female']\n",
    "roles = ['user', 'admin']\n",
    "departments = ['Administration', 'Delivery', 'Operations']\n",
    "user_statuses = ['active', 'left']\n",
    "designations = ['Business Administrator', 'CEO', 'Consultant', 'Intern', 'Manager', 'Senior Associate Consultant', 'Senior Consultant', 'Solutions Consultant', 'Software Engineer', 'Solution Enabler', 'Senior Software Engineer']\n",
    "designation_ids = {designation: fake.uuid4() for designation in designations}\n",
    "\n",
    "# Generate fixed IDs for each department\n",
    "department_ids = {department: fake.uuid4() for department in departments}\n",
    "# Function to generate user ID\n",
    "def generate_user_id():\n",
    "    return 'JMAN' + str(random.randint(1, 999))\n",
    "\n",
    "# Function to generate mobile number\n",
    "def generate_mobile_number():\n",
    "    return str(random.randint(1000000000, 9999999999))\n",
    "\n",
    "# Function to generate email\n",
    "def generate_email(username):\n",
    "    domains = ['gmail.com', 'yahoo.com']\n",
    "    return username + '@' + random.choice(domains)\n",
    "\n",
    "# Function to generate pin\n",
    "def generate_pin():\n",
    "    return str(random.randint(100000, 999999))\n",
    "\n",
    "# Function to generate reporting user ID from the pool of generated user IDs\n",
    "def generate_reporting_user_id(user_ids):\n",
    "    if user_ids:\n",
    "        return random.choice(user_ids)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to generate random user data\n",
    "def generate_user_data(csv_writer, user_ids):\n",
    "    user_id = generate_user_id()\n",
    "    username = fake.name()\n",
    "    mobile_number = generate_mobile_number()\n",
    "    email = generate_email(username.split()[0])\n",
    "    state = random.choice(states)\n",
    "    pin = generate_pin()\n",
    "    gender = random.choice(genders)\n",
    "    reporting_user_id = generate_reporting_user_id(user_ids)\n",
    "    while reporting_user_id == user_id:  # Ensure reporting user ID is not same as user ID\n",
    "        reporting_user_id = generate_reporting_user_id(user_ids)\n",
    "    reporting_name = fake.name()\n",
    "    role = random.choice(roles)\n",
    "    designation = random.choice(designations)\n",
    "    department = random.choice(departments)\n",
    "    user_status = random.choice(user_statuses)\n",
    "    password = fake.password()\n",
    "    require_password_change = random.choice([True, False])\n",
    "    date_of_join = fake.date_between_dates(date_start=datetime.date(2010, 1, 1), date_end=datetime.date(2024, 12, 31)).strftime('%d/%m/%Y')\n",
    "    date_of_birth = fake.date_of_birth(minimum_age=21, maximum_age=50).strftime('%d/%m/%Y')\n",
    "    user_data = {\n",
    "        'user_id': user_id,\n",
    "        'userName': username,\n",
    "        'dateOfJoin': date_of_join,\n",
    "        'mobileNumber': mobile_number,\n",
    "        'email': email,\n",
    "        'city': fake.city(),\n",
    "        'state': state,\n",
    "        'pin': pin,\n",
    "        'dateOfBirth': date_of_birth,\n",
    "        'gender': gender,\n",
    "        'reportingUserId': reporting_user_id,\n",
    "        'reporterName': reporting_name,\n",
    "        'roles': role,\n",
    "        'designationId':  designation_ids[designation],\n",
    "        'designation': designation,\n",
    "        'departmentId': department_ids[department],\n",
    "        'department': department,\n",
    "        'user_status': user_status,\n",
    "        'password': password,\n",
    "        'requirePasswordChange': require_password_change\n",
    "        \n",
    "    }\n",
    "    return user_data\n",
    "    # csv_writer.writerow(user_data)\n",
    "\n",
    "# Generate 1000 user data\n",
    "user_data_list = [generate_user_data([], []) for _ in range(1000)]\n",
    "\n",
    "# Extract user IDs for reporting\n",
    "reporting_user_ids = [user['user_id'] for user in user_data_list]\n",
    "\n",
    "# Generate 30 unique reporting user IDs from the pool of 1000 user IDs\n",
    "selected_reporting_user_ids = random.sample(reporting_user_ids, 30)\n",
    "\n",
    "# Generate user data\n",
    "for i in range(1000):\n",
    "    user_data = generate_user_data(writer, selected_reporting_user_ids)\n",
    "    user_data_list.append(user_data)\n",
    "\n",
    "# Print sample data\n",
    "for i in range(5):\n",
    "    print(user_data_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('user_details.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['user_id', 'userName', 'dateOfJoin', 'mobileNumber', 'email', 'city', 'state', 'pin',\n",
    "                  'dateOfBirth', 'gender', 'reportingUserId', 'reporterName', 'roles', 'designationId',\n",
    "                  'designation', 'departmentId', 'department', 'user_status', 'password', 'requirePasswordChange']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Generate and write 1000 user data directly to CSV\n",
    "    for user_data in user_data_list:\n",
    "        writer.writerow(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define lists of data for enums\n",
    "statuses = ['active', 'in progress', 'completed']\n",
    "types_of_project = ['data engineering', 'data science', 'full stack', 'web development']\n",
    "departments = {\n",
    "    'Administration': '9ca17918-0e85-4e94-b2ba-298f26eaf27e',\n",
    "    'Delivery': 'b738be0a-f6bb-4c02-882a-c7ac1cabdb01',\n",
    "    'Operations': '5e79cd96-279f-45cf-9984-5d822a2632e1'\n",
    "}\n",
    "\n",
    "# Function to generate project department and department ID\n",
    "def generate_department_details():\n",
    "    department = random.choice(list(departments.keys()))\n",
    "    department_id = departments[department]\n",
    "    return department, department_id\n",
    "\n",
    "# Function to generate project ID\n",
    "def generate_project_id():\n",
    "    return 'PROJ' + str(random.randint(10000, 99999))\n",
    "\n",
    "# Function to generate project name\n",
    "def generate_project_name():\n",
    "    return fake.company()\n",
    "\n",
    "# Function to generate project start date\n",
    "def generate_start_date():\n",
    "    today = datetime.date.today()\n",
    "    start_date = today - datetime.timedelta(days=random.randint(90, 730))  # 3 months to 2 years ago\n",
    "    return start_date.strftime('%d/%m/%Y')\n",
    "\n",
    "# Function to generate project end date\n",
    "def generate_end_date(start_date):\n",
    "    start_date = datetime.datetime.strptime(start_date, '%d/%m/%Y')\n",
    "    end_date = start_date + datetime.timedelta(days=random.randint(90, 730))  # 3 months to 2 years from start date\n",
    "    return end_date.strftime('%d/%m/%Y')\n",
    "\n",
    "# Function to generate project status\n",
    "def generate_project_status():\n",
    "    return random.choice(statuses)\n",
    "\n",
    "# Function to generate project type\n",
    "def generate_project_type():\n",
    "    return random.choice(types_of_project)\n",
    "\n",
    "# Function to generate project data\n",
    "def generate_project_data():\n",
    "    project_id = generate_project_id()\n",
    "    project_name = generate_project_name()\n",
    "    start_date = generate_start_date()\n",
    "    end_date = generate_end_date(start_date)\n",
    "    project_status = generate_project_status()\n",
    "    type_of_project = generate_project_type()\n",
    "    department, department_id = generate_department_details()\n",
    "\n",
    "    project_data = {\n",
    "        'project_id': project_id,\n",
    "        'project_name': project_name,\n",
    "        'project_startDate': start_date,\n",
    "        'project_endDate': end_date,\n",
    "        'project_status': project_status,\n",
    "        'typeOfProject': type_of_project,\n",
    "        'departmentId': department_id,\n",
    "        'department': department\n",
    "    }\n",
    "    return project_data\n",
    "\n",
    "# Generate project data\n",
    "project_data_list = [generate_project_data() for _ in range(120)]\n",
    "\n",
    "# Print sample data\n",
    "for i in range(5):\n",
    "    print(project_data_list[i])\n",
    "\n",
    "# Write project data to CSV\n",
    "with open('project_details.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['project_id', 'project_name', 'project_startDate', 'project_endDate', 'project_status',\n",
    "                  'typeOfProject', 'departmentId', 'department']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(project_data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df = pd.read_csv('project_details.csv')\n",
    "\n",
    "tasks=[\n",
    "  \"Implement user authentication\",\n",
    "  \"Design homepage layout\",\n",
    "  \"Build ETL pipeline for data ingestion\",\n",
    "  \"Train machine learning model for sentiment analysis\",\n",
    "  \"Develop RESTful API endpoints\",\n",
    "  \"Optimize database queries for performance\",\n",
    "  \"Create responsive UI for mobile devices\",\n",
    "  \"Deploy application to cloud platform\",\n",
    "  \"Implement OAuth 2.0 authentication\",\n",
    "  \"Write unit tests for backend services\",\n",
    "  \"Design database schema for scalability\",\n",
    "  \"Analyze user behavior data for insights\",\n",
    "  \"Refactor legacy codebase for maintainability\",\n",
    "  \"Integrate third-party APIs for additional functionality\",\n",
    "  \"Build real-time data processing system\",\n",
    "  \"Create interactive data visualizations\",\n",
    "  \"Implement multi-factor authentication\",\n",
    "  \"Develop CRUD operations for managing data\",\n",
    "  \"Automate deployment process using CI/CD pipeline\",\n",
    "  \"Optimize application for scalability and performance\",\n",
    "  \"Perform code reviews and provide feedback\",\n",
    "  \"Implement caching mechanism for improved performance\",\n",
    "  \"Design and implement continuous integration workflow\",\n",
    "  \"Create documentation for APIs and data models\",\n",
    "  \"Build recommendation system based on user preferences\",\n",
    "  \"Implement error handling and logging\",\n",
    "  \"Develop microservices architecture\",\n",
    "  \"Create batch processing system for large datasets\",\n",
    "  \"Design user interface wireframes and mockups\",\n",
    "  \"Deploy application using containerization technologies\",\n",
    "  \"Optimize application for search engine visibility\",\n",
    "  \"Implement data encryption and security measures\",\n",
    "  \"Develop asynchronous task processing system\",\n",
    "  \"Design and implement user permission management system\",\n",
    "  \"Build data pipeline for real-time analytics\",\n",
    "  \"Create automated tests for frontend components\",\n",
    "  \"Implement version control system for source code management\",\n",
    "  \"Develop natural language processing algorithms\",\n",
    "  \"Design and implement API rate limiting\",\n",
    "  \"Integrate payment gateway for e-commerce functionality\",\n",
    "  \"Build scalable infrastructure using cloud services\",\n",
    "  \"Create data validation rules and constraints\",\n",
    "  \"Implement single sign-on functionality\",\n",
    "  \"Develop user registration and profile management system\",\n",
    "  \"Design and implement distributed system architecture\",\n",
    "  \"Optimize application for cross-browser compatibility\",\n",
    "  \"Create data migration scripts for database upgrades\",\n",
    "  \"Implement role-based access control system\",\n",
    "  \"Develop data cleaning and preprocessing pipelines\",\n",
    "  \"Design and implement data storage solutions\",\n",
    "  \"Build authentication and authorization service\",\n",
    "  \"Integrate chatbot functionality into application\",\n",
    "  \"Create automated deployment scripts\",\n",
    "  \"Implement data anonymization techniques\",\n",
    "  \"Develop user interface components using React.js\",\n",
    "  \"Design and implement task scheduling system\",\n",
    "  \"Build geospatial data analysis tools\",\n",
    "  \"Optimize application for mobile performance\",\n",
    "  \"Implement session management and expiration\",\n",
    "  \"Develop recommendation engine based on collaborative filtering\",\n",
    "  \"Design and implement event-driven architecture\",\n",
    "  \"Create custom admin dashboard for monitoring\",\n",
    "  \"Build data visualization dashboard using D3.js\",\n",
    "  \"Implement content management system\",\n",
    "  \"Develop API documentation using OpenAPI specification\",\n",
    "  \"Design and implement logging and monitoring system\",\n",
    "  \"Build user engagement tracking system\",\n",
    "  \"Implement file upload and storage functionality\",\n",
    "  \"Develop serverless computing solutions\",\n",
    "  \"Design and implement user feedback collection system\",\n",
    "  \"Optimize database indexing for query performance\",\n",
    "  \"Create user interface animations and transitions\",\n",
    "  \"Implement email notification system\",\n",
    "  \"Develop custom analytics tracking solution\",\n",
    "  \"Design and implement data aggregation pipelines\",\n",
    "  \"Build integration tests for system components\",\n",
    "  \"Implement data replication and synchronization\",\n",
    "  \"Develop real-time collaboration features\",\n",
    "  \"Design and implement task prioritization system\",\n",
    "  \"Create data reporting and visualization tools\",\n",
    "  \"Build data archiving and retrieval system\",\n",
    "  \"Optimize application for accessibility compliance\",\n",
    "  \"Implement data backup and disaster recovery measures\",\n",
    "  \"Develop A/B testing framework for experimentation\",\n",
    "  \"Design and implement user onboarding process\",\n",
    "  \"Build data governance framework\",\n",
    "  \"Implement continuous monitoring and alerting\",\n",
    "  \"Develop content recommendation system\",\n",
    "  \"Design and implement data transformation pipelines\",\n",
    "  \"Create data quality assurance procedures\",\n",
    "  \"Build notification system for alerts and updates\",\n",
    "  \"Optimize application for low-latency communication\",\n",
    "  \"Implement user session management and persistence\",\n",
    "  \"Develop data segmentation and targeting algorithms\",\n",
    "  \"Design and implement user role management system\",\n",
    "  \"Build data migration tooling for seamless upgrades\",\n",
    "  \"Implement feature toggle system for controlled rollouts\",\n",
    "  \"Develop user interface accessibility features\",\n",
    "  \"Design and implement data classification system\",\n",
    "  \"Create data privacy and compliance policies\",\n",
    "  \"Build automated regression testing suite\",\n",
    "  \"Implement user activity tracking and analytics\",\n",
    "  \"Develop data governance policies and procedures\",\n",
    "  \"Design and implement distributed caching system\",\n",
    "  \"Build user-friendly error handling and feedback mechanisms\",\n",
    "  \"Optimize application for high availability and fault tolerance\",\n",
    "  \"Implement data retention and deletion policies\",\n",
    "  \"Develop user engagement measurement metrics\",\n",
    "  \"Design and implement data encryption at rest\",\n",
    "  \"Create data lineage and audit trail mechanisms\",\n",
    "  \"Build data validation and cleansing toolset\",\n",
    "  \"Implement user-friendly data import and export features\",\n",
    "  \"Develop data quality monitoring and reporting capabilities\",  \"Create data access control mechanisms\",\n",
    "  \"Build automated data validation and reconciliation processes\",\n",
    "  \"Implement real-time data replication across distributed systems\",\n",
    "  \"Develop user-centric error reporting and resolution workflows\",\n",
    "  \"Design and implement data access logging and auditing\",\n",
    "  \"Build anomaly detection system for data integrity monitoring\",\n",
    "  \"Implement user-driven data privacy settings and controls\",\n",
    "  \"Develop data lifecycle management workflows\",\n",
    "  \"Design and implement data cataloging and discovery capabilities\",\n",
    "  \"Create data-driven anomaly detection algorithms\",\n",
    "  \"Build automated data quality assessment and improvement tools\",\n",
    "  \"Implement user-friendly data visualization customization options\",\n",
    "  \"Develop data versioning and lineage tracking mechanisms\",\n",
    "  \"Design and implement data lineage visualization tools\",\n",
    "  \"Build real-time data validation and correction pipelines\",\n",
    "  \"Implement data provenance tracking for compliance and auditing\",\n",
    "  \"Develop self-service data exploration and analysis platforms\",\n",
    "  \"Design and implement data anonymization and de-identification techniques\",\n",
    "  \"Create data-driven anomaly detection dashboards\",\n",
    "  \"Build automated data classification and tagging systems\",\n",
    "  \"Implement user-driven data access request workflows\",\n",
    "  \"Develop data privacy impact assessment frameworks\",\n",
    "  \"Design and implement data encryption in transit mechanisms\",\n",
    "  \"Build data breach detection and response systems\",\n",
    "  \"Implement real-time data security monitoring and alerting\",\n",
    "  \"Develop user-centric data governance dashboards\",\n",
    "  \"Design and implement data governance policy enforcement mechanisms\",\n",
    "  \"Create data-driven risk assessment and mitigation strategies\",\n",
    "  \"Build automated data quality exception handling workflows\",\n",
    "  \"Implement user-friendly data lineage visualization tools\",\n",
    "  \"Develop data usage tracking and reporting capabilities\",\n",
    "  \"Design and implement data retention policy management systems\",\n",
    "  \"Build automated data validation and correction mechanisms\",\n",
    "  \"Implement real-time data synchronization across heterogeneous systems\",\n",
    "  \"Develop data quality profiling and anomaly detection algorithms\",\n",
    "  \"Design and implement data quality assessment and improvement workflows\",\n",
    "  \"Create user-friendly data privacy compliance dashboards\",\n",
    "  \"Build automated data access request approval workflows\",\n",
    "  \"Implement real-time data access monitoring and logging\",\n",
    "  \"Develop user-driven data access control policies\",\n",
    "  \"Design and implement data masking and obfuscation techniques\",\n",
    "  \"Build anomaly detection and prediction models for data integrity monitoring\",\n",
    "  \"Implement user-friendly data classification and labeling interfaces\",\n",
    "  \"Develop data lineage visualization and exploration tools\",\n",
    "  \"Design and implement data governance policy management platforms\",\n",
    "  \"Create data-driven data governance policy recommendation engines\",\n",
    "  \"Build automated data quality assessment and remediation pipelines\",\n",
    "  \"Implement real-time data anomaly detection and response mechanisms\",\n",
    "  \"Develop user-centric data privacy compliance training modules\",\n",
    "  \"Design and implement data access audit trail visualization tools\",\n",
    "  \"Build automated data privacy impact assessment frameworks\",\n",
    "  \"Implement user-driven data access certification workflows\",\n",
    "  \"Develop data privacy compliance reporting and monitoring dashboards\",\n",
    "  \"Design and implement data retention policy enforcement mechanisms\",\n",
    "  \"Create user-friendly data breach detection and response dashboards\",\n",
    "  \"Build automated data security incident response workflows\",\n",
    "  \"Implement real-time data breach detection and notification systems\",\n",
    "  \"Develop user-centric data governance policy management dashboards\",\n",
    "  \"Design and implement data privacy compliance policy enforcement mechanisms\",\n",
    "  \"Build anomaly detection and prediction models for data security monitoring\",\n",
    "  \"Implement user-friendly data access certification and attestation workflows\",\n",
    "  \"Develop data privacy impact assessment and mitigation planning tools\",\n",
    "  \"Design and implement data breach response and recovery procedures\",\n",
    "  \"Create data-driven data privacy compliance training and awareness programs\",\n",
    "  \"Build automated data breach detection and notification workflows\",\n",
    "  \"Implement real-time data security incident response and remediation mechanisms\",\n",
    "  \"Develop user-centric data privacy compliance reporting and monitoring dashboards\",\n",
    "  \"Design and implement data governance policy compliance assessment frameworks\",\n",
    "  \"Create user-friendly data access certification and attestation dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data privacy monitoring\",\n",
    "  \"Implement automated data privacy impact assessment and mitigation workflows\",\n",
    "  \"Develop real-time data breach response and recovery orchestration platforms\",\n",
    "  \"Design and implement data privacy compliance auditing and attestation mechanisms\",\n",
    "  \"Build automated data governance policy compliance assessment and reporting pipelines\",\n",
    "  \"Implement user-friendly data breach response and recovery coordination workflows\",\n",
    "  \"Develop data privacy compliance regulatory compliance and certification frameworks\",\n",
    "  \"Design and implement data privacy compliance policy exception management systems\",\n",
    "  \"Create user-friendly data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data governance monitoring\",\n",
    "  \"Implement automated data privacy compliance policy exception management workflows\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting platforms\",\n",
    "  \"Design and implement data privacy compliance policy exception management approval workflows\",\n",
    "  \"Create automated data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data management monitoring\",\n",
    "  \"Implement automated data governance policy compliance assessment and reporting pipelines\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Design and implement data governance policy compliance assessment and reporting approval workflows\",\n",
    "  \"Create user-friendly data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data quality monitoring\",\n",
    "  \"Implement automated data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting approval workflows\",\n",
    "  \"Design and implement data governance policy compliance assessment and reporting platform\",\n",
    "  \"Create automated data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data security monitoring\",\n",
    "  \"Implement automated data governance policy compliance assessment and reporting pipelines\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting platforms\",\n",
    "  \"Design and implement data governance policy compliance assessment and reporting approval workflows\",\n",
    "  \"Create user-friendly data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data privacy monitoring\",\n",
    "  \"Implement automated data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting approval workflows\",\n",
    "  \"Design and implement data governance policy compliance assessment and reporting platforms\",\n",
    "  \"Create automated data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Build anomaly detection and prediction models for data management monitoring\",\n",
    "  \"Implement automated data governance policy compliance assessment and reporting pipelines\",\n",
    "  \"Develop real-time data governance policy compliance assessment and reporting dashboards\",\n",
    "  \"Design and implement data governance policy compliance assessment and reporting approval workflows\",\n",
    "  \"Create user-friendly data governance policy compliance assessment and reporting dashboards\"\n",
    "]\n",
    "\n",
    "# Function to generate fake tasks\n",
    "global_tasks = tasks\n",
    "def generate_fake_tasks(num_tasks):\n",
    "    fake = Faker()\n",
    "    tasks = []\n",
    "    for _ in range(num_tasks):\n",
    "        task = {\n",
    "            'projectId': projectId,\n",
    "            'task': random.choice(global_tasks), # Use the global tasks list\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "\n",
    "# Generate fake tasks for each project\n",
    "all_tasks = []\n",
    "for projectId in projects_df['project_id']:\n",
    "    num_tasks = random.randint(3, 5)  # Generate 3-5 tasks per project\n",
    "    tasks = generate_fake_tasks(num_tasks)\n",
    "    all_tasks.extend(tasks)\n",
    "\n",
    "# Convert the list of tasks to DataFrame\n",
    "tasks_df = pd.DataFrame(all_tasks)\n",
    "\n",
    "# Store the tasks into another CSV file\n",
    "tasks_df.to_csv('tasks_details.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details = pd.read_csv('project_details.csv')\n",
    "\n",
    "# Load user details from CSV\n",
    "user_details = pd.read_csv('user_details.csv')\n",
    "\n",
    "# Define designations and their corresponding limits\n",
    "designations = {\n",
    "    'Consultant': 5,\n",
    "    'Intern': 1,\n",
    "    'Manager': 2,\n",
    "    'Senior Associate Consultant': 4,\n",
    "    'Senior Consultant': 3,\n",
    "    'Solutions Consultant': 4,\n",
    "    'Software Engineer': 8,\n",
    "    'Solution Enabler': 6,\n",
    "    'Senior Software Engineer': 5\n",
    "}\n",
    "\n",
    "# Open the CSV file in append mode for resource allocations\n",
    "with open('resource_allocations.csv', 'a') as resource_file:\n",
    "    # Write the header to the CSV file if the file is empty\n",
    "    if resource_file.tell() == 0:\n",
    "        resource_file.write(\"projectId,userId,startDate,endDate\\n\")\n",
    "\n",
    "    # Function to assign resources to projects\n",
    "    def assign_resources(project_id, start_date, end_date):\n",
    "        allocated_users = set()\n",
    "        active_users = user_details[user_details['user_status'] == 'active']\n",
    "        for designation, limit in designations.items():\n",
    "            available_users = active_users[(active_users['designation'] == designation) & (~active_users['user_id'].isin(allocated_users))]\n",
    "            if available_users.empty:\n",
    "                print(f\"No available active users for designation {designation} in project {project_id}\")\n",
    "                continue\n",
    "            num_assignments = min(len(available_users), limit)\n",
    "            selected_users = random.sample(list(available_users['user_id']), num_assignments)\n",
    "            for user_id in selected_users:\n",
    "                allocated_users.add(user_id)\n",
    "                # Ensure the allocation date is at least 2 months from the start date\n",
    "                min_allocation_date = start_date + timedelta(days=60)  # 60 days is approximately 2 months\n",
    "                max_allocation_date = end_date - timedelta(days=1)  # Ensure it's before the project end date\n",
    "                allocation_date = datetime.date.fromordinal(random.randint(min_allocation_date.toordinal(), max_allocation_date.toordinal()))\n",
    "                # Calculate the end date to be at least 2 months after the start date\n",
    "                allocation_end_date = allocation_date + timedelta(days=60)  # 60 days is approximately 2 months\n",
    "                # Write the allocation to the CSV file\n",
    "                resource_file.write(f\"{project_id},{user_id},{allocation_date.strftime('%d/%m/%y')},{allocation_end_date.strftime('%d/%m/%y')}\\n\")\n",
    "                resource_file.flush()# Ensure the data is written to the file immediately\n",
    "    \n",
    "    # Iterate over each project and assign resources\n",
    "    for index, row in project_details.iterrows():\n",
    "        # Check if the project status is 'active' or 'in progress'\n",
    "        project_status = row['project_status']\n",
    "        if project_status not in ['active', 'in progress']:\n",
    "            print(f\"Skipping project {row['project_id']} as it is not active or in progress.\")\n",
    "            continue\n",
    "        \n",
    "        # Correctly parse the dates based on the format in the CSV\n",
    "# Correctly parse the dates based on the format in the CSV\n",
    "        start_date = datetime.datetime.strptime (row['project_startDate'], '%d/%m/%Y')\n",
    "        end_date = datetime.datetime.strptime(row['project_endDate'], '%d/%m/%Y')\n",
    "        assign_resources(row['project_id'], start_date, end_date)\n",
    "\n",
    "print(\"Resource allocations have been appended to resource_allocations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import random\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# Load project details from CSV\n",
    "project_details = pd.read_csv('project_details.csv')\n",
    "\n",
    "# Load resource allocations from CSV\n",
    "resource_allocations = pd.read_csv('resource_allocations.csv')\n",
    "\n",
    "# Load task details from CSV\n",
    "task_details = pd.read_csv('tasks_details.csv')\n",
    "\n",
    "# Define statuses\n",
    "statuses = ['submit', 'save']\n",
    "\n",
    "two_word_comments = [\n",
    "    \"Progress Update\", \"Task Completed\", \"Bug Fixed\", \"Code Review\", \"Design Updated\",\n",
    "    \"Meeting Scheduled\", \"Deadline Extended\", \"Feature Implemented\", \"Testing Passed\",\n",
    "    \"Deployment Ready\", \"Documentation Revised\", \"Requirement Clarified\", \"Issue Resolved\",\n",
    "    \"Data Analysis\", \"User Feedback\", \"Performance Optimization\", \"Security Patch\",\n",
    "    \"Infrastructure Setup\"]\n",
    "\n",
    "# Function to generate random comments\n",
    "def generate_comments():\n",
    "    return random.choice(two_word_comments)\n",
    "\n",
    "# Open the CSV file in append mode\n",
    "with open('timesheet_data.csv', 'a') as file:\n",
    "    # Write the header to the CSV file\n",
    "    file.write(\"userId,startDate,endDate,category,rowId,projectId,taskId,comments,column_0,column_1,column_2,column_3,column_4,column_5,column_6,status\\n\")\n",
    "    \n",
    "    # Iterate over each project allocation and assign resources\n",
    "    for index, row in resource_allocations.iterrows():\n",
    "        project_id = row['projectId']\n",
    "        user_id = row['userId']\n",
    "        start_date_val = datetime.datetime.strptime(row['startDate'], '%d/%m/%y')\n",
    "        end_date_val = datetime.datetime.strptime(row['endDate'], '%d/%m/%y')\n",
    "        \n",
    "        current_date = start_date_val\n",
    "        while current_date < end_date_val:\n",
    "            # Initialize row_id for each week\n",
    "            row_id = 1\n",
    "            \n",
    "            # Calculate the start and end dates for each week\n",
    "            week_start_date = current_date - timedelta(days=current_date.weekday()) # Monday of the week\n",
    "            week_end_date = week_start_date + timedelta(days=6) # Sunday of the week\n",
    "            \n",
    "            # Check if the user has any projects allocated for this week\n",
    "            user_projects_this_week = resource_allocations[(resource_allocations['userId'] == user_id) & \n",
    "                                                            (resource_allocations['startDate'] <= week_end_date.strftime('%Y-%m-%d')) & \n",
    "                                                            (resource_allocations['endDate'] >= week_start_date.strftime('%Y-%m-%d'))]\n",
    "            \n",
    "            if not user_projects_this_week.empty:\n",
    "                # Generate random number of rows for each entry, between 2 and 5\n",
    "                num_rows = random.randint(2, 5)\n",
    "                for _ in range(num_rows):\n",
    "                    # Select a random project for this entry\n",
    "                    project_id = random.choice(user_projects_this_week['projectId'].tolist())\n",
    "                    project_tasks = task_details[task_details['projectId'] == project_id]\n",
    "                    if not project_tasks.empty:\n",
    "                        # Ensure the 'task' column exists in the project_tasks DataFrame\n",
    "                        if 'task' in project_tasks.columns:\n",
    "                            task_id = random.choice(project_tasks['task'].tolist()) # Convert the Series to a list before choosing a random task\n",
    "                        else:\n",
    "                            print(f\"Error: 'task' column not found in project_tasks for project ID {project_id}\")\n",
    "                            continue # Skip this iteration if the 'task' column is missing\n",
    "                        comments = generate_comments() # Generate random comments\n",
    "                        \n",
    "                        # Generate random values for each column\n",
    "                        column_values = [random.randint(0,12) for _ in range(7)]\n",
    "                        # Ensure sum of all columns is between 20 and 70\n",
    "                        while sum(column_values) < 20 or sum(column_values) > 60:\n",
    "                            column_values = [random.randint(0, 12) for _ in range(7)]\n",
    "                        # Write each row to the CSV file\n",
    "                        file.write(f\"{user_id},{week_start_date.strftime('%d-%m-%Y')},{week_end_date.strftime('%d-%m-%Y')},{project_details[project_details['project_id'] == project_id]['category'].iloc[0]},{row_id},{project_id},{task_id},{comments},{','.join(map(str, column_values))},{random.choice(statuses)}\\n\")\n",
    "\n",
    "                        # Append after creating each record\n",
    "                        file.flush()\n",
    "                        row_id += 1\n",
    "            else:\n",
    "                # If no projects allocated for this week, assign default values\n",
    "                column_values = [random.randint(0,12) for _ in range(7)] # Generate random values for each column\n",
    "                file.write(f\"{user_id},{week_start_date.strftime('%d-%m-%Y')},{week_end_date.strftime('%d-%m-%Y')},Bench,{row_id},bench001,,{','.join(map(str, column_values))},{random.choice(statuses)}\\n\")\n",
    "                file.flush()\n",
    "                row_id += 1\n",
    "            # Move to the next week\n",
    "            current_date += timedelta(weeks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB connection parameters\n",
    "uri = \"mongodb+srv://Mohana:mohana123@user.11jyqrm.mongodb.net/?retryWrites=true&w=majority&appName=user\"\n",
    "client = MongoClient(uri)\n",
    "db = client['FinalProjData']\n",
    "\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = ['user_details.csv', 'project_details.csv', 'resource_allocations.csv', 'tasks_details.csv','timesheet_data.csv']\n",
    "col_list = ['user_details', 'project_details', 'resource_allocations', 'tasks_details','timesheet_data']\n",
    "# Iterate over each CSV file\n",
    "for csv_file in range(len(csv_files)):\n",
    "    collection = db[col_list[csv_file]]\n",
    "    # Load CSV data into a DataFrame\n",
    "    df = pd.read_csv(csv_files[csv_file])\n",
    "\n",
    "    # Convert DataFrame to dictionary\n",
    "    data_dict = df.to_dict(orient='records')\n",
    "\n",
    "    # Insert data into MongoDB collection\n",
    "    collection.insert_many(data_dict)\n",
    "\n",
    "# Close MongoDB connection\n",
    "client.close()\n",
    "\n",
    "print(\"CSV data from multiple files pushed to MongoDB Atlas successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
